# é›™å‘åŒæ­¥ GitHub è³‡æ–™çš„è…³æœ¬

library(tidyverse)
library(gh)
library(git2r)

# è¨­å®š GitHub èªè­‰
setup_github_auth <- function() {
  token <- Sys.getenv("GITHUB_PAT")
  if (token == "") {
    # å˜—è©¦å¾ keyring å–å¾—
    tryCatch({
      token <- keyring::key_get("github_pat")
      Sys.setenv(GITHUB_PAT = token)
    }, error = function(e) {
      stop("è«‹è¨­å®š GITHUB_PAT ç’°å¢ƒè®Šæ•¸æˆ–ä½¿ç”¨ keyring::key_set('github_pat')")
    })
  }
}

# åˆ—å‡ºæ‰€æœ‰é æ¸¬ repositories
list_forecast_repos <- function() {
  setup_github_auth()
  
  repos <- gh("GET /orgs/{org}/repos",
              org = "cdc-modeling-hub",
              type = "all",
              per_page = 100)
  
  # ç¯©é¸é æ¸¬ repos
  forecast_repos <- keep(repos, 
                        ~ grepl("^forecast-", .x$name))
  
  map_chr(forecast_repos, "name")
}

# ä¸‹è¼‰ç‰¹å®šæª”æ¡ˆ
download_forecast <- function(repo_name, file_path) {
  setup_github_auth()
  
  content <- gh("GET /repos/{owner}/{repo}/contents/{path}",
                owner = "cdc-modeling-hub",
                repo = repo_name,
                path = file_path)
  
  # Base64 è§£ç¢¼
  raw_content <- base64enc::base64decode(content$content)
  temp_file <- tempfile(fileext = ".csv")
  writeBin(raw_content, temp_file)
  
  read_csv(temp_file, show_col_types = FALSE)
}

# å¾ GitHub æ‹‰å–æœ€æ–°è³‡æ–™
pull_from_github <- function(local_repo_path = "./local_repo") {
  if (!dir.exists(local_repo_path)) {
    cat("æœ¬åœ° repository ä¸å­˜åœ¨ï¼Œè«‹å…ˆ clone\n")
    return(FALSE)
  }
  
  repo <- repository(local_repo_path)
  
  # æ‹‰å–æ›´æ–°
  setup_github_auth()
  fetch(repo, credentials = cred_token())
  
  # æª¢æŸ¥æ˜¯å¦æœ‰æ›´æ–°
  local_sha <- sha(repository_head(repo))
  
  tryCatch({
    merge(repo, "origin/main")
    
    new_sha <- sha(repository_head(repo))
    
    if (local_sha != new_sha) {
      cat("âœ… ç™¼ç¾æ–°çš„æ›´æ–°ï¼Œå·²åˆä½µ\n")
      return(TRUE)
    } else {
      cat("ğŸ“„ æ²’æœ‰æ–°çš„æ›´æ–°\n")
      return(FALSE)
    }
    
  }, error = function(e) {
    cat("âŒ åˆä½µå¤±æ•—:", e$message, "\n")
    return(FALSE)
  })
}

# è™•ç†æ–°é æ¸¬è³‡æ–™
process_new_forecasts <- function(repo_path = "./local_repo") {
  cat("è™•ç†æ–°çš„é æ¸¬è³‡æ–™...\n")
  
  # å°‹æ‰¾æ–°çš„ CSV æª”æ¡ˆ
  csv_files <- list.files(
    file.path(repo_path, "data-processed"),
    pattern = "\\.csv$",
    recursive = TRUE,
    full.names = TRUE
  )
  
  all_forecasts <- data.frame()
  
  for (file in csv_files) {
    tryCatch({
      forecast_data <- read_csv(file, show_col_types = FALSE)
      forecast_data$source_file <- basename(file)
      all_forecasts <- bind_rows(all_forecasts, forecast_data)
    }, error = function(e) {
      cat("ç„¡æ³•è®€å–æª”æ¡ˆ", file, ":", e$message, "\n")
    })
  }
  
  if (nrow(all_forecasts) > 0) {
    cat("è™•ç†äº†", nrow(all_forecasts), "ç­†é æ¸¬è³‡æ–™\n")
    return(all_forecasts)
  } else {
    cat("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é æ¸¬è³‡æ–™\n")
    return(NULL)
  }
}

# åŸ·è¡Œé›†æˆåˆ†æ (ç¤ºä¾‹)
run_ensemble_analysis <- function(forecast_data = NULL) {
  if (is.null(forecast_data)) {
    cat("æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æ\n")
    return(NULL)
  }
  
  cat("åŸ·è¡Œé›†æˆåˆ†æ...\n")
  
  # ç°¡å–®çš„é›†æˆæ–¹æ³• - è¨ˆç®—å¹³å‡å€¼
  ensemble_results <- forecast_data %>%
    filter(type == "point") %>%
    group_by(forecast_date, target, target_end_date, location) %>%
    summarise(
      ensemble_value = mean(value, na.rm = TRUE),
      model_count = n(),
      .groups = "drop"
    ) %>%
    mutate(
      type = "point",
      quantile = NA_real_,
      model = "ensemble"
    )
  
  cat("âœ… é›†æˆåˆ†æå®Œæˆï¼Œç”¢ç”Ÿ", nrow(ensemble_results), "ç­†çµæœ\n")
  return(ensemble_results)
}

# æ¨é€çµæœå› GitHub
push_results_to_github <- function(results, repo_path = "./local_repo") {
  if (is.null(results) || nrow(results) == 0) {
    cat("æ²’æœ‰çµæœå¯æ¨é€\n")
    return(FALSE)
  }
  
  repo <- repository(repo_path)
  
  # å„²å­˜çµæœ
  results_dir <- file.path(repo_path, "analysis")
  dir.create(results_dir, showWarnings = FALSE)
  
  timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
  results_file <- file.path(results_dir, paste0("ensemble_results_", timestamp, ".csv"))
  
  write_csv(results, results_file)
  
  # Git æ“ä½œ
  tryCatch({
    add(repo, paste0("analysis/ensemble_results_", timestamp, ".csv"))
    
    commit(repo, 
           message = paste("Update ensemble analysis", timestamp),
           author = signature("R Server", "rserver@cdc.gov.tw"))
    
    setup_github_auth()
    push(repo, credentials = cred_token())
    
    cat("âœ… çµæœå·²æ¨é€åˆ° GitHub\n")
    return(TRUE)
    
  }, error = function(e) {
    cat("âŒ æ¨é€å¤±æ•—:", e$message, "\n")
    return(FALSE)
  })
}

# ä¸»åŒæ­¥å‡½æ•¸
sync_cycle <- function(repo_path = "./local_repo") {
  cat("é–‹å§‹åŒæ­¥é€±æœŸ...\n")
  cat("æ™‚é–“:", format(Sys.time()), "\n")
  
  # 1. å¾ GitHub æ‹‰å–æ›´æ–°
  has_updates <- pull_from_github(repo_path)
  
  if (has_updates) {
    # 2. è™•ç†æ–°è³‡æ–™
    forecast_data <- process_new_forecasts(repo_path)
    
    # 3. åŸ·è¡Œåˆ†æ
    results <- run_ensemble_analysis(forecast_data)
    
    # 4. æ¨é€çµæœ
    push_results_to_github(results, repo_path)
  }
  
  cat("âœ… åŒæ­¥é€±æœŸå®Œæˆ\n")
  cat("ä¸‹æ¬¡åŒæ­¥æ™‚é–“:", format(Sys.time() + 3600), "\n")  # 1å°æ™‚å¾Œ
}

# è¨­å®šå®šæœŸåŒæ­¥
setup_sync_schedule <- function() {
  library(cronR)
  
  script_path <- system.file("scripts/sync_with_github.R", package = getwd())
  log_path <- "/home/rserver/logs/github_sync.log"
  
  cmd <- cron_rscript(
    rscript = script_path,
    rscript_log = log_path,
    log_append = TRUE
  )
  
  # æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡
  cron_add(
    command = cmd,
    frequency = "hourly",
    id = "github_sync",
    description = "æ¯å°æ™‚åŒæ­¥ GitHub è³‡æ–™"
  )
  
  cat("âœ… å·²è¨­å®šæ¯å°æ™‚åŒæ­¥æ’ç¨‹\n")
}

# ä¸»åŸ·è¡Œå€å¡Š
if (!interactive()) {
  # åŸ·è¡Œä¸€æ¬¡åŒæ­¥é€±æœŸ
  sync_cycle()
}